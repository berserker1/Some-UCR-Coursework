{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKc8K9_57UEz"
      },
      "source": [
        "## CS224 - Fall 2024\n",
        "## PROGRAMMING ASSIGNMENT 1 - Principal Component Analysis (PCA) & K-means Clustering\n",
        "\n",
        "### Due: November 5, 2024 @ 11:59pm PDT\n",
        "\n",
        "**Submission Method**: Submit both the .ipynb and the PDF file on **Gradescope**. (For more details, see the Assignment Guidelines.)\n",
        "\n",
        "**Maximum points**: 15"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JL5tIX4c9z6s"
      },
      "source": [
        "<div style=\"margin-bottom: 15px; padding: 15px; color: #31708f; background-color: #d9edf7; border: 1px solid #bce8f1; border-radius: 5px;\">\n",
        "    \n",
        "<b><font size=+2>Enter your information below:</font></b></br></br>\n",
        "\n",
        "  <b>(full) Name</b>: [Enter Your Name here]\n",
        "  </br>\n",
        "\n",
        "  <b>Student ID Number</b>:  [Enter Your SID here]\n",
        "  </br></br>\n",
        "    \n",
        "<b>By submitting this notebook, I assert that the work below is my own work, completed for this course.  Except where explicitly cited, none of the portions of this notebook are duplicated from anyone else's work or my own previous work.</b>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coxduSVB9gut"
      },
      "source": [
        "<div style=\"padding: 15px; color: #8a6d3b; background-color: #fcf8e3; border: 1px solid #faebcc; border-radius: 5px;\">\n",
        "<b><font size=+2>Academic Integrity</font></b></br>\n",
        "Each assignment should be done  individually. You may discuss general approaches with other students in the class, and ask questions to the TA, but  you must only submit work that is yours . If you receive help by any external sources (other than the TA and the instructor), you must properly credit those sources. The UCR Academic Integrity policies are available at <a href=\"http://conduct.ucr.edu/policies/academicintegrity.html\" target=\"_blank\">http://conduct.ucr.edu/policies/academicintegrity.html</a>.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StEpjy4JuSYA"
      },
      "source": [
        "# Overview\n",
        "In this assignment, We will implement PCA, apply it to the [**MNIST**](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html) dataset, and observe how the reconstruction changes as we change the number of principal components used.\n",
        "\n",
        "For this assignment we will use the functionality of [Numpy](http://www.numpy.org/), and [Matplotlib](https://matplotlib.org/).\n",
        "\n",
        "*   Before you start, make sure you have installed all those packages in your local Jupyter instance.\n",
        "*   If you are asked to implement a particular functionality, you should **not** use an existing implementation from the libraries above (or some other library that you may find). When in doubt, **please just ASK**.\n",
        "*   It's okay to use functions in `numpy.linalg` to calculate matrix decomposition (e.g., `la.eig()`, `la.svd()`), but using built-in functions like `sklearn.decomposition.PCA()` will **not** get you any points.\n",
        "\n",
        "\n",
        "Please read **all** cells carefully and answer **all** parts (both text and missing code). You will need to complete all the code marked `YOUR CODE HERE` and answer descriptive/derivation questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zGqZM8CXCEnG"
      },
      "outputs": [],
      "source": [
        "!pip install numpy\n",
        "!pip install matplotlib\n",
        "!pip install scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NM7-Cx4H-jTt"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_digits\n",
        "\n",
        "np.random.seed(42)\n",
        "# DO NOT REMOVE THE CODE ABOVE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHgsgFGF-kNA"
      },
      "source": [
        "## Question 1 [8 points]\n",
        "\n",
        "**Preliminaries**\n",
        "\n",
        "The [**MNIST**](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html) database (Modified National Institute of Standards and Technology database) is a large database of handwritten digits that is commonly used for training various image processing systems.\n",
        "\n",
        "First, Let's import the images and vectorize each image in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DnLvNYtwuSYC"
      },
      "outputs": [],
      "source": [
        "# Data Preparation\n",
        "mnist = load_digits()\n",
        "data = mnist.data\n",
        "# Display the shape of the data\n",
        "print(\"Data shape:\", data.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_F8GgYQuSYC"
      },
      "source": [
        "**(a) [1 point]**  Compute the mean and variance of the images and standardize the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G_aPdQuouSYC"
      },
      "outputs": [],
      "source": [
        "# Step 1: Standardize the Data\n",
        "def standardize(data):\n",
        "    \"\"\"\n",
        "    Standardize the dataset.\n",
        "\n",
        "    Parameters:\n",
        "    data (numpy array): Original data array.\n",
        "\n",
        "    Returns:\n",
        "    standardized_data (numpy array): Data after standardization.\n",
        "    \"\"\"\n",
        "    # TODO: Compute the mean and standard deviation of the data.\n",
        "    mean = # YOUR CODE HERE\n",
        "    std = np.std(data, axis=0) + 1e-10 # 1e-10 added to avoid division by zero error\n",
        "\n",
        "    # TODO: Return the standardized data.\n",
        "    standardized_data = # YOUR CODE HERE\n",
        "\n",
        "    return standardized_data, mean, std\n",
        "\n",
        "# Standardize the data and print the first 2 rows\n",
        "data_standardized, mean, std = standardize(data)\n",
        "print(\"Shape of data_standardized: \", data_standardized.shape)\n",
        "print(\"Shape of mean vector: \", mean.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kP-cSD3XmDHD"
      },
      "source": [
        "**(b) [1 point]** Calculate the covariance matrix for the standardized dataset and calculate the eigenvalues and eigenvectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r7OMyx89kYnC"
      },
      "outputs": [],
      "source": [
        "# Step 2: Calculate the covariance matrix for the features in the dataset.\n",
        "def compute_covariance_matrix(data):\n",
        "    \"\"\"\n",
        "    Compute the covariance matrix of the standardized data.\n",
        "\n",
        "    Parameters:\n",
        "    data (numpy array): Standardized data array.\n",
        "\n",
        "    Returns:\n",
        "    covariance_matrix (numpy array): Covariance matrix of the data.\n",
        "    \"\"\"\n",
        "    covariance_matrix = # YOUR CODE HERE;\n",
        "    # You can use the numpy function for this; however, if you write your own code to implement it and show that the result is similar to what numpy provides, you can get 1 extra point credit.\n",
        "    return covariance_matrix\n",
        "\n",
        "# Compute the covariance matrix\n",
        "covariance_matrix = compute_covariance_matrix(data_standardized)\n",
        "print(\"Shape of Covariance Matrix\", covariance_matrix.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MiCZCqr0o-QO"
      },
      "outputs": [],
      "source": [
        "def compute_eigen(covariance_matrix):\n",
        "    \"\"\"\n",
        "    Compute the eigenvalues and eigenvectors of the covariance matrix.\n",
        "\n",
        "    Parameters:\n",
        "    covariance_matrix (numpy array): Covariance matrix of the data.\n",
        "\n",
        "    Returns:\n",
        "    eigenvalues (numpy array): Eigenvalues in descending order.\n",
        "    eigenvectors (numpy array): Corresponding eigenvectors.\n",
        "    \"\"\"\n",
        "\n",
        "    # Step 3: Calculate the eigenvalues and eigenvectors for the covariance matrix.\n",
        "    eigenvalues, eigenvectors = # YOUR CODE HERE\n",
        "\n",
        "    # Step 4: Sort eigenvalues and their corresponding eigenvectors (in descending order.)\n",
        "    sorted_indices = # YOUR CODE HERE\n",
        "    eigenvalues = # YOUR CODE HERE\n",
        "    eigenvectors = # YOUR CODE HERE\n",
        "\n",
        "    return eigenvalues, eigenvectors\n",
        "\n",
        "# Compute eigenvalues and eigenvectors\n",
        "eigenvalues, eigenvectors = compute_eigen(covariance_matrix)\n",
        "print(\"Shape of Eigenvalues: \", eigenvalues.shape)\n",
        "print(\"Shape of Eigenvectors:\", eigenvectors.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywadWSHsYEZd"
      },
      "source": [
        "**(c) [2 points]** Analyze the eigenvalues in $\\Lambda$ and decide which eigenvalues to retain and which can be set to zero.\n",
        "\n",
        "* You may want to plot the eigenvalues, the fraction of variance explained, AIC, or BIC to help decide on a threshold.\n",
        "* Ensure your plots are clearly labeled with titles and axes labels, which is critical for understanding the visualized data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p2-zIA5vYOHA"
      },
      "outputs": [],
      "source": [
        "def draw(eigenvalues, eigenvectors):\n",
        "    # TODO: Add your plotting code here\n",
        "    # Example plots you may consider include:\n",
        "    # - Plotting the eigenvalues\n",
        "    # - Plotting the cumulative explained variance\n",
        "    # - Any additional analysis (AIC, BIC, etc.)\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    pass\n",
        "\n",
        "draw(eigenvalues, eigenvectors)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9UHZEggYKd0"
      },
      "source": [
        "**(d) [1 point]**  Reconstruct an approximation of each X after removing some of the small eigenvalues. (Display only a couple of the reconstructed **images**, and you will need to restore the reconstructed data to its original scale using the mean and standard deviation.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9IXcSJM96m2"
      },
      "outputs": [],
      "source": [
        "# Step 5: Pick k eigenvalues and form a matrix of eigenvectors.\n",
        "def find_optimal_k(eigenvalues, eigenvectors):\n",
        "    \"\"\"\n",
        "    Analyze eigenvalues and decide which to retain.\n",
        "\n",
        "    Parameters:\n",
        "    eigenvalues (numpy array): Eigenvalues in descending order.\n",
        "    eigenvectors (numpy array): Corresponding eigenvectors.\n",
        "\n",
        "    Returns:\n",
        "    n_components (int): Number of principal components to retain.\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO: Determine the number of components needed to reach the threshold.\n",
        "    # YOUR CODE HERE\n",
        "    n_components = # YOUR CODE HERE\n",
        "    return n_components\n",
        "\n",
        "def choose_principal_components(n_components, eigenvectors):\n",
        "    \"\"\"\n",
        "    Choose the principal components based on the number selected.\n",
        "\n",
        "    Parameters:\n",
        "    n_components (int): Number of components to retain.\n",
        "    eigenvectors (numpy array): The original eigenvectors.\n",
        "\n",
        "    Returns:\n",
        "    selected_eigenvectors (numpy array): Eigenvectors corresponding to the top k components.\n",
        "    \"\"\"\n",
        "    selected_eigenvectors = # YOUR CODE HERE\n",
        "    return selected_eigenvectors\n",
        "\n",
        "n_components = find_optimal_k(eigenvalues, eigenvectors)\n",
        "selected_eigenvectors = choose_principal_components(n_components, eigenvectors)\n",
        "print(f\"Selected {n_components} eigenvectors.\")\n",
        "print(f\"Shape of selected eigenvectors: {selected_eigenvectors.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "265qk4h2YUrc"
      },
      "outputs": [],
      "source": [
        "# Step 6: Reconstruct Data from Principal Components\n",
        "def reconstruct_data(data_standardized, selected_eigenvectors):\n",
        "    \"\"\"\n",
        "    Reconstruct the original data using selected eigenvectors.\n",
        "\n",
        "    Parameters:\n",
        "    data_standardized (numpy array): The standardized data matrix.\n",
        "    selected_eigenvectors (numpy array): Eigenvectors corresponding to the top k components.\n",
        "\n",
        "    Returns:\n",
        "    data_reconstructed (numpy array): Reconstructed approximation of the original data.\n",
        "    \"\"\"\n",
        "    # Project the original data onto the selected principal components\n",
        "    data_projected = # YOUR CODE HERE\n",
        "\n",
        "    # Reconstruct the data by projecting back into the original space\n",
        "    data_reconstructed = # YOUR CODE HERE\n",
        "\n",
        "    return data_reconstructed\n",
        "\n",
        "def restore_original_scale(X_reconstructed, mean_vector, std_vector):\n",
        "    \"\"\"\n",
        "    Restore the reconstructed data to its original scale using the mean and standard deviation.\n",
        "\n",
        "    Parameters:\n",
        "    X_reconstructed (numpy array): Reconstructed data (standardized scale).\n",
        "    mean_vector (numpy array): Mean vector used during standardization.\n",
        "    std_vector (numpy array): Standard deviation vector used during standardization.\n",
        "\n",
        "    Returns:\n",
        "    X_restored (numpy array): Reconstructed data in its original scale.\n",
        "    \"\"\"\n",
        "    X_restored = # YOUR CODE HERE\n",
        "\n",
        "    return X_restored\n",
        "\n",
        "def display_reconstruct_data(data_standardized, data_reconstructed, num_examples=4):\n",
        "    image_shape = (8, 8)\n",
        "    random_indices = np.random.choice(data_standardized.shape[0], size=num_examples, replace=False)\n",
        "\n",
        "    for idx in random_indices:\n",
        "        plt.figure(figsize=(4, 2))\n",
        "\n",
        "        # Original image\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.imshow(data_standardized[idx].reshape(image_shape), cmap='gray')\n",
        "        plt.title(\"Original\")\n",
        "        plt.axis('off')\n",
        "\n",
        "        # Reconstructed image\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.imshow(data_reconstructed[idx].reshape(image_shape), cmap='gray')\n",
        "        plt.title(\"Reconstructed\")\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "# Transform the data using the selected components\n",
        "data_reconstructed = reconstruct_data(data_standardized, selected_eigenvectors)\n",
        "print(\"Shape of reconstructed data:\", data_reconstructed.shape)\n",
        "\n",
        "display_reconstruct_data(restore_original_scale(data_standardized, mean, std),\n",
        "                         restore_original_scale(data_reconstructed, mean, std))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zeK-Qoc0TXu"
      },
      "source": [
        "**(e) [2 points]**  Compute the error between the reconstructed X and original image. (The mean of the original data should **not** be included in the error.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "O5Hn63Id0TXu"
      },
      "outputs": [],
      "source": [
        "def compute_reconstruction_error(data_standardized, data_reconstructed):\n",
        "    \"\"\"\n",
        "    Compute the mean squared error (MSE) between the standardized original data\n",
        "    and the reconstructed data.\n",
        "\n",
        "    Parameters:\n",
        "    data_standardized (numpy array): The standardized original data matrix.\n",
        "    data_reconstructed (numpy array): Reconstructed approximation of the standardized data.\n",
        "\n",
        "    Returns:\n",
        "    error (float): Mean squared error between original and reconstructed data.\n",
        "    \"\"\"\n",
        "    # TODO: Calculate the MSE between the standardized original data and reconstructed data\n",
        "    error = # YOUR CODE HERE\n",
        "\n",
        "    return error\n",
        "\n",
        "reconstruction_error = compute_reconstruction_error(data_standardized, data_reconstructed)\n",
        "print(f\"Reconstruction error (MSE): {reconstruction_error:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8BLgPQQ0TXv"
      },
      "source": [
        "**(f) [1 points]**  Analyze by choosing different numbers of eigenvalues to be zeroed out. Provide a short summary of your conclusions based on this analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "wvkiFOUP0TXv"
      },
      "outputs": [],
      "source": [
        "# TODO: YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVMtQGE8MWlO"
      },
      "source": [
        "[TODO: YOUR SUMMARY HERE]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjNx9zLHEvN0"
      },
      "source": [
        "## Question 2 [7 points]\n",
        "After implementing PCA, use k-means clustering on the PCA-transformed data.\n",
        "* To enable effective visualization in a 2D space, we choose the first 2 components of your PCA-transformed data for further (visual) analysis.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d97PkCdiQdXN"
      },
      "outputs": [],
      "source": [
        "def transform_data_with_pca(data_standardized, eigenvectors, n_components):\n",
        "    X = data_standardized @ choose_principal_components(n_components, eigenvectors)\n",
        "    return X\n",
        "\n",
        "X = transform_data_with_pca(data_standardized, eigenvectors, n_components=2)\n",
        "print(\"Shape of X: \", X.shape)\n",
        "\n",
        "true_labels = mnist.target\n",
        "print(\"Shape of true_labels: \", true_labels.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hFDh4jCQdpA"
      },
      "source": [
        "**(a)[3 points]** Implement k-means from scratch. Please refrain from using libraries like `scikit-learn` for the k-means functionality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tpeHqQatEvN0"
      },
      "outputs": [],
      "source": [
        "# Apply K-means Clustering to PCA-transformed Data\n",
        "def k_means(X, k, max_iter=300):\n",
        "    \"\"\"\n",
        "    Implement K-means clustering from scratch.\n",
        "\n",
        "    Parameters:\n",
        "    X (numpy array): Data to be clustered (n_samples, n_features).\n",
        "    k (int): Number of clusters.\n",
        "    max_iter (int): Maximum number of iterations for convergence.\n",
        "\n",
        "    Returns:\n",
        "    cluster_labels (numpy array): Cluster labels for each data point.\n",
        "    centroids (numpy array): Coordinates of the cluster centers.\n",
        "    \"\"\"\n",
        "    np.random.seed(0)  # For reproducibility\n",
        "    # Randomly initialize the centroids by selecting k random samples from X\n",
        "    centroids = # YOUR CODE HERE\n",
        "\n",
        "    for iteration in range(max_iter):\n",
        "        # Assign each data point to the nearest centroid\n",
        "        # YOUR CODE HERE\n",
        "        labels = # YOUR CODE HERE\n",
        "\n",
        "        # Calculate new centroids as the mean of assigned points\n",
        "        new_centroids = # YOUR CODE HERE\n",
        "\n",
        "        # Check for convergence (if centroids don't change)\n",
        "        if np.all(centroids == new_centroids):\n",
        "            break\n",
        "\n",
        "        centroids = new_centroids\n",
        "\n",
        "    return labels, centroids\n",
        "\n",
        "\n",
        "k = 10 # Number of clusters (digits 0-9)\n",
        "cluster_labels, centroids = k_means(X, k)\n",
        "print(\"Shape of centroids: \", centroids.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVQLQhV6EvN0"
      },
      "source": [
        "Plot the clustering results on the PCA-reduced data using different colors for each cluster(digit)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "omHyJTvgEvN0"
      },
      "outputs": [],
      "source": [
        "# Visualize Clusters\n",
        "def plot_clusters(data, labels, title):\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    plt.scatter(data[:, 0], data[:, 1], c=labels, cmap='viridis', alpha=0.4)\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Component 1')\n",
        "    plt.ylabel('Component 2')\n",
        "    plt.colorbar(label='Cluster')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Plot the ground truth clusters\n",
        "plot_clusters(X, true_labels, \"True Labels\")\n",
        "# Plot the predicted clusters on PCA-transformed data\n",
        "plot_clusters(X, cluster_labels, \"Predicted Clusters (K-means)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTNgcQ3IEvN0"
      },
      "source": [
        "After applying k-means on the PCA-reduced data, let's evaluate how well the clustering algorithm performed by checking the accuracy for each digit."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbCYPo_wTEOF"
      },
      "source": [
        "**(b)[3 points]** Analyze the accuracy of clustering for each digit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLqofd7_UGAt"
      },
      "outputs": [],
      "source": [
        "def analyze_clustering_accuracy(true_labels, cluster_labels, num_clusters):\n",
        "    \"\"\"\n",
        "    Analyze the accuracy of clustering for each digit based on true labels.\n",
        "\n",
        "    Parameters:\n",
        "    - true_labels: array-like, shape (n_samples,)\n",
        "        The true labels of the samples (0-9 for digits).\n",
        "    - cluster_labels: array-like, shape (n_samples,)\n",
        "        The predicted cluster labels from k-means.\n",
        "    - num_clusters: int\n",
        "        The number of clusters (should match the number of unique digits, usually 10).\n",
        "\n",
        "    Returns:\n",
        "    - accuracy_per_digit: dict\n",
        "        A dictionary mapping each digit to its accuracy.\n",
        "    - total_accuracy: float\n",
        "        The overall accuracy of the clustering.\n",
        "    - cluster_to_digit: dict\n",
        "        A dictionary mapping each cluster to the assigned digit class.\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize accuracy dictionary for digits 0-9\n",
        "    accuracy_per_digit = {digit: 0.0 for digit in range(10)}\n",
        "\n",
        "    # Initialize total correct predictions\n",
        "    total_correct_predictions = 0\n",
        "    total_samples = len(true_labels)\n",
        "\n",
        "    # Create a mapping from cluster to digit\n",
        "    cluster_to_digit = {}\n",
        "\n",
        "    # Count correct predictions for each cluster\n",
        "    for cluster in range(num_clusters):\n",
        "        cluster_mask = (cluster_labels == cluster)  # Mask for the current cluster\n",
        "        if np.any(cluster_mask):  # Only proceed if the cluster has members\n",
        "            predicted_labels = true_labels[cluster_mask]  # True labels for this cluster\n",
        "            # TODO: Find the most common true label in this cluster\n",
        "            most_common_label = # YOUR CODE HERE\n",
        "            cluster_to_digit[cluster] = most_common_label  # Map the cluster to the most common label\n",
        "\n",
        "            # Update total correct predictions\n",
        "            total_correct_predictions += np.sum(predicted_labels == most_common_label)  # Count correct predictions\n",
        "\n",
        "            # Accumulate accuracy for the most common label\n",
        "            accuracy_per_digit[most_common_label] += np.sum(predicted_labels == most_common_label)\n",
        "\n",
        "    # Normalize the accuracy by the number of instances for each digit\n",
        "    for digit in range(10):\n",
        "        total_count = np.sum(true_labels == digit)\n",
        "        if total_count > 0:\n",
        "            accuracy_per_digit[digit] /= total_count\n",
        "\n",
        "    # Calculate total accuracy\n",
        "    total_accuracy = total_correct_predictions / total_samples if total_samples > 0 else 0.0\n",
        "\n",
        "    # Assign new predicted labels based on the cluster-to-digit mapping\n",
        "    final_predicted_labels = np.vectorize(cluster_to_digit.get)(cluster_labels)\n",
        "\n",
        "    return accuracy_per_digit, total_accuracy, cluster_to_digit, final_predicted_labels\n",
        "\n",
        "# TODO: Try changing this value to see how it affects the clustering\n",
        "n_components = # YOUR CODE HERE\n",
        "\n",
        "X = transform_data_with_pca(data_standardized, eigenvectors, n_components=n_components)\n",
        "\n",
        "cluster_labels, centroids = k_means(X, k)\n",
        "accuracy_per_digit, total_accuracy, cluster_to_digit, final_predicted_labels = analyze_clustering_accuracy(true_labels, cluster_labels, num_clusters=k)\n",
        "\n",
        "print(\"n_components: \", n_components)\n",
        "print(\"Shape of X: \", X.shape)\n",
        "[print(f\"Digit {digit}: {accuracy:.4f}\") for digit, accuracy in accuracy_per_digit.items()];\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GoSIfTYsV_WX"
      },
      "outputs": [],
      "source": [
        "def plot_accuracy_per_digit(accuracy_per_digit):\n",
        "    \"\"\"\n",
        "    Plot a bar chart for the accuracy of each digit.\n",
        "\n",
        "    Parameters:\n",
        "    - accuracy_per_digit: dict\n",
        "        A dictionary mapping each digit to its accuracy.\n",
        "    \"\"\"\n",
        "    digits = list(accuracy_per_digit.keys())\n",
        "    accuracies = list(accuracy_per_digit.values())\n",
        "\n",
        "    plt.figure(figsize=(6, 3))\n",
        "    plt.bar(digits, accuracies, color='skyblue')\n",
        "    plt.xlabel('Digits')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Clustering Accuracy per Digit')\n",
        "    plt.xticks(digits)  # Ensure each digit is shown on the x-axis\n",
        "    plt.ylim(0, 1)  # Set y-axis limits to [0, 1]\n",
        "    plt.grid(axis='y')  # Add grid lines for better readability\n",
        "    plt.show()\n",
        "\n",
        "plot_accuracy_per_digit(accuracy_per_digit)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtOaB2xCEvN0"
      },
      "source": [
        "**(c)[1 point]** Provide a short summary of your observations based on this analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAx8rwv-jg7d"
      },
      "source": [
        "[TODO: YOUR SUMMARY HERE]"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    },
    "vscode": {
      "interpreter": {
        "hash": "50292dbb1f747f7151d445135d392af3138fb3c65386d17d9510cb605222b10b"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
